# N-grams
This is a sequence of N words.
This is the equation for Bi-grams:
$$p(w_n|w_{n-1}) = \frac{C(w_{n-1}w_n)}{\sum_wC(w_{n-1}w)} ==  \frac{C(w_{n-1}w_n)}{\sum_wC(w_{n-1})}$$
For general N-grams
$$p(w_n|w_{n-N+1}) = \frac{C(w_{n-N+1:n-1}w_n)}{\sum_wC(w_{n-N+1}w)} $$
### Example
![[Screenshot 2024-02-13 at 21.07.45.png]]
Given the information above, we can calculate the different conditional probabilities:
$P(Sam| am) = \frac{1}{2}$   $P(am| I) = \frac{2}{3}$   $P(I|<s>) = \frac{2}{3}$  $P(Sam|<s>) = \frac{1}{3}$

We can create a bi-gram table, and to find the probability of the next word. And use the MLE to estimate the most likely word.
![[Screenshot 2024-02-13 at 21.12.56.png]]

# Recurrent Neural Network (RNN)
In a simple RNN, it contains a network which has a cycle- it reuses an earlier output as input. Therefore the activation value of the hidden layer depends on the current input, but also the activation value of the hidden layer from the previous time step.
![[Screenshot 2024-02-13 at 21.24.39.png]]

For any given $t$, we get the word embedding of the word, $w_t$. We combine $e_t$ and the previous hidden representation, $h_{t-1}$, to get $Vh$.  Then you can calculate probability distribution score over the vocabulary, using softmax, which is $y_t$. The one that has the highest probability is the next word.

![[Screenshot 2024-02-13 at 21.30.52.png]]

What is the general procedure of training an RNN as a language model:
- It is done using a corpus, model learns to predict the next word at each time step t
- Model minimises the error in predicting the correct next word.
- Error/Loss: it is the difference between a predicated probability distribution and the correct distribution i.e. cross-entropy loss.
- The correct distribution is one-hot encoded.

There are few properties of RNN:
- **Teacher forcing**: The model is given the correct history sequence(rather than what was predicted in the previous time step)
- **Gradient Descent**: It is used to adjust the weights in the RNN< in order to minimise the CE loss average over the sequence.
![[Screenshot 2024-02-13 at 21.53.46.png]]
- **Autoregressive generation**: The word generated at each time step, $t$, is conditioned on the word selected by the RNN from the previous time step $t-1$Applications for machine translation, summarisation.
![[Screenshot 2024-02-13 at 21.55.28.png]]


Key difference between n-gram and RNN language models: 
- N-gram language models incorporate information from **limited context** only (preceding n-1 tokens) 
- RNN language models: the hidden state can **incorporate information from all the preceding words**


