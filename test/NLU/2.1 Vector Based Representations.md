## Embeddings
It is **learned representation** of the meaning of words based on vector semantics
- The meaning is observed by the distribution of a word in language use
- Words with similar contexts, tend to have similar meaning
![[Screenshot 2024-02-13 at 18.04.16.png]]

# Sparse vector representations
There are two main types:
- Term Frequency-Inverse Document Frequency (TF-IDF)
- Positive Pointwise Mutual Information(PPMI)
## TF-IDF
Term frequency: number of times a word appears in a given document
![[Screenshot 2024-02-13 at 18.09.05.png]]

IDF: measures how **informative** a term is. It is based on the `document frequency`, which is then umber of documents in which a term appears.
![[Screenshot 2024-02-13 at 18.10.20.png]]

Then you can create a TF-IDF table:
![[Screenshot 2024-02-13 at 18.11.13.png]]

## PPMI
This is based on the `word co-occurance` matrix; takes into account the extent of which two words co-occur.

PPMI is derived from PPI, which is how often a target word, `w`, and a context word, `c`, co-occur compared when they occur independently.
$$PMI(w,c) = \log_2 \frac{P(w,c)}{P(w)P(c)}$$
PMI estimates how much more two words co-occur compared to it being by change. However, this can be negative to positive infinity

Instead, we use 
$$PPMI(w,c) = \max (\log_2 \frac{p_{ij}}{P_*(i)P_*(j)}, 0)$$
where
$$p_{ij} = \frac{f_{ij}}{\sum^W_{i=1}\sum^C_{j=1} f_{ij}}$$
The denominator is the same as summing all the columns and rows.
And
$$p_*(i)=\frac{\sum^C_{j=1} f_{ij}}{\sum^W_{i=1}\sum^C_{j=1} f_{ij}}$$
$$p_*(j)=\frac{\sum^W_{j=1} f_{ij}}{\sum^W_{i=1}\sum^C_{j=1} f_{ij}}$$
#### Example:
Suppose you are given this table:
![[Screenshot 2024-02-13 at 18.29.32.png]]
Suppose we are trying to find the PPMI of $PPMI_{information, data}$ 
$$p(w=information,c=data)  = \frac{f_{ij}}{\sum^W_{i=1}\sum^C_{j=1} f_{ij}} = \frac{3982}{11716} = 0.3399$$
$$p_*(i)= p(w=information)=\frac{\sum^C_{j=1} f_{ij}}{\sum^W_{i=1}\sum^C_{j=1} f_{ij}} = \frac{7703}{11716} = 0.6575$$
$$p_*(j)=P(c=data) = \frac{\sum^W_{j=1} f_{ij}}{\sum^W_{i=1}\sum^C_{j=1} f_{ij}} = \frac{5673}{11716} = 0.4842$$
$$PPMI_{Information,Data} = \max(log_2\frac{0.3399}{0.6575 * 0.4842},0) = 0.0944$$
![[Screenshot 2024-02-13 at 18.39.25.png]]


# Dense-Vector Representation
There are 3 big ones:
- word2vec
- GloVe
- fastText
## Word2vec
There are two types:
1) Skip-gram: model learns to predict the context of the word given the target word.
2) CBOW: model learn to predict the most likely target word given the context.

## GloVe

