We reply on humans to label or annotate data, but humans have different perspectives.
We use **annotator agreement** which helps us decide whether we can trust the labels.
There are two types:
- **Intra-annotator agreement**: This is where the same human consistently annotates the same item when presented at different times.
- **Inter-annotator agreement**(IAA): This is where multiple humans are consistently able to annotate the same item even when working independently 

Why use IAA?
- It serves as an indication of the **difficulty** of the task or how well defined the task is. 
- It also serves as an **upper bound** on. the performance of automated methods.

### Observed Agreement
This is a very simplistic approach. It takes into account the ratio of items they agree to the total number of items. 
This biggest flaw is it doesn't take into account the **probability** of it being the same.
![[Screenshot 2024-02-27 at 10.58.46.png]]
In this case: 3 out of 5 times they're in agreement. This equates to 0.6. However:
- both A1 & A2 randomly choose $Y=0.5 \times 0.5 =0.25$
- both A1 & A2 randomly chose $N = 0.5 \times 0.5 = 0.25$ 
- hence expected agreement is 0.5, which is slightly higher than 0.6.

### Cohen's Kappa coefficients
It is a measure of chance-corrected agreement:
$$\kappa = \frac{P(a)-P(e)}{1-P(e)}$$
$P(a)$ ~ **observed agreement**, proportion of times annotators agreed.
$P(e)$ ~ **expected agreement**, proportion of times annotators expected to agree by chance.
![[Screenshot 2024-02-27 at 11.08.49.png]]
[Example of Cohen's Kappa](obsidian://open?vault=Obsidian%20Vault&file=NLU%2FAttachments%2FReliability%20of%20Data%20and%20Reporting%20Performance%20AY2023-24.pdf#page=7)
Generally: a negative value means disagreement; 0 means no agreement.
> slight < 0.2 < fair < 0.4 < moderate < 0.6 < substantial < 0.8 < perfect
> 0.67 < tentative conclusion < 0.8 < definite conclusion

There are different ways to find coefficients for IAA: 
- **Scott's Pi**: P(e) assigns different chance for different categories.
- **Fleiss' Kappa**: multi-annotator generalisation of (Cohen's) Kappa and Scott's Pi

For some task, coefficients from IAA doesn't make a lot of sense- especially if you not sure what a negative case is. Suppose you negative value for NER, what does this mean?  
- You can have token-level and entity level which may be hard to define what the agreement, and disagreement. 
- It could also be difficult to define what the chance of it is.
For such tasks we use the **F-score** to calculate IAA:
- The annotations from one of the annotators is considered as **gold standard** (reference) 
- The annotations from another annotator is considered as **response**, whose F-score is measured against the reference

# Evaluation Metics
Typical Reporting summarised results of evaluations are:
![[Screenshot 2024-02-27 at 12.34.44.png]]

![[Screenshot 2024-02-27 at 12.35.55.png]]
The tables compares the different methods, but we need to consider are these differences significant! It is important to check that this is statistically significant. 

### Statistical Significance 
**Null hypothesis statistical testing:** can be performed on two samples of data, e.g., different accuracy values
- **Null hypothesis:** that there is no difference between the distribution of the two samples of data (i.e., that any variation is due to chance) 
- Statistical test: many different types, depending on whether (1) the data distribution is normal or not, and (2) the data is paired or not

|                                 | Unpaired                                  | Paired                              |
| ------------------------------- | ----------------------------------------- | ----------------------------------- |
| Parametric(Normal dist)         | Independent t-test (Student's or Welsh's) | Paired t-test(Student's or Welch's) |
| Non-parametric(Non-Normal dist) | Mann-Whitney U Test                       | Welcoxon signed -ranted test        |
results in a `p-value`: if below a threshold, then the null hypothesis is rejected.


