Evaluation is `testing`; we want to assess which model/method produces expected output.

Evaluation:
- Manual vs Automatic
- Formative vs Summative
- Intrinsic vs Extrinsic
- Component vs End-to-end
# Automatic vs Manual
**Manual evaluation** involves using human subjects who will asses the system's output.
Limitations: human inconsistencies, difficult to control for external factors, time-consuming and laborious 

**Automatic evaluation**, is data driven, and requires algorithms mimicking human assessors.

Using Hate Speech Detection Problem:
Manual: A human would need to check the output of the system for every tweet.
Automatic: Evaluation script measures performance metrics for classification problems. 
# Formative vs Summative
**Formative Evaluation** occurs during the development of systems. Hence it is mainly used to inform the developer if progress has been made towards the system's goals.
- Usually lightweights and iterative - so the feedback can be added quickly
- Tends to be **automatic.**

**Summative Evaluation** is conducted after the system completion; often involves `human judges`. They assess if the system's goals were reached.

Using Hate Speech Detection Problem:
Formative: automatic measurement(using test data) of classification metrics every time some improvement to the model is incorporated.
Summative: demonstrating the final model to users, applying it to both a test set and ideally new data.

# Intrinsic & Extrinsic 
**Intrinsic Evaluation:** This is where the assessment is done in terms of the system's underlying/internal task.

**Extrinsic Evaluation**: This is where the assessment is done in terms of the impact of the system on an external task(application of the system)

Using Hate Speech Detection Problem:
Intrinsic: How well does the sequence classification model perform?
Extrinsic: How much faster a human able to carry out content moderation?

# Component vs End-to-End
**Component Evaluation**: Assessing each component comprising a pipeline. This allows for the `isolation` of errors and identifying problematic components.

**End-to-End**: Assessing all components at once. This provides an indication of a system's effectiveness under real-world conditions.

Using Hate Speech Detection Problem:
Component: Separating pre-processing/cleaning from classification 
End-to-End: Measure classification performance given raw-text

# Preparing Data for Evaluation
How do we ensure that the data is ready for evaluation:
- Using fixed parition/split
- k-fold CV

Common Issues:
- Sampling Bias
- Imbalanced Data
### Fixed Partition Data:
**Training** set: usually the bulk of the entire data
**Held-out** set: often divided into 
- `development/validation(dev)` set: for fine tuning (hyper)parameters 
- `development-test (dev-test) `set: for formative evaluation 
**Test set**: for summative evaluation after system development

The subsets should be **disjoint**. Also the results can be **optimistic** if the test data is drawn from the same data distribution. 
Limitations:
- lucky partition?
- small data set?

### k-Fold CV
How does it work?
- Data is split into `k` folds 
- For each round `i` in `k` rounds: all folds except fold `i` are used for training, and fold `i` for testing 
- If parameter tuning is needed, a small part of the training folds is held out T
Benefit: k can be set to a large value in order to use more data for training

### Sampling Bias
This occurs if samples included in the test data set doesn't represent the true distribution. This can be solved by using **stratified random sampling**.
- A representative number of instances are randomly drawn from each strata
- ensures each class is sufficiently represented.
![[Screenshot 2024-02-27 at 09.50.36.png]]

### Imbalanced data
This is where datasets have some classes that are over and under represented.

We can deal  with this using **random under-sampling**(basic).
- Randomly selecting what to remove or keep from the majority class
- Can lead to **loss** of information
![[Screenshot 2024-02-27 at 09.55.57.png|700]]

Under-sampling using **Tomek Links**(advanced):
This removes pairs of instances from the opposite class which are very similar to each other.
![[Screenshot 2024-02-27 at 09.59.24.png]]

Random **over-sampling**(basic):
This makes copies of the minority class instances. However this can lead to overfitting.
![[Screenshot 2024-02-27 at 10.01.05.png]]


**Synthetic Minority Over-Sampling(SMOTE)**: 
This creates new instances from the minority class. 
- It does this by randomly selecting an instance $m$ and find the $k$ nearest neighbours(NNs) to it. 
- One $n$ from NN is chosen out of the $k$ randomly.
- Then a synthetic instance is generated by taking the convex combination of $m$ and $n$.
![[Screenshot 2024-02-27 at 10.08.12.png| 700]]

