Before you start there are different types of documents, and these need to be dealt with. You need to format these files to the correct type. This is known as **data cleansing**.

**Language Identification**: how do know the natural language of the documents.
- Use the most frequent words
- HTML tags
- Character set in use!

It is also important to consider the **genre** of the documents. The **tokenisation** might depend on the context of the documents e.g. a full stop could mean the end of a sentence, or multiplication in maths.

## Tokenisation
Tokenisation is breaking up **basic inputs of texts** into **token** to be processed.
- An easy way is to use white spaces, however not all languages can be separated using white space. 
- Another issue is with separating with punctuation marks e.g. $\frac{1}{2}$ is one but 200g/7oz parsnips.
- Different directions of reading depending on the language.

### Other issues:
- Names of places: San Francisco?
	- Do we need to know that it is one things?
	- Might be easier to have this as two token.
- Hewlett-Packard? 
	- Well this is one entity.
	- It would make sense to have this as one token.
- what're, I'm $\longrightarrow$ what + are, I + am
	- mono-form? so you know what the sentence means.
	- how many tokens do you want?
- King's coming $\longrightarrow$ King + is + coming
	- Could have different meaning if they're separated
	- You could have tokens, and sub-tokens.

#### Typical Tokenisation Steps:
- Initial segmentation - separating using white space.
- Handling abbreviations and apostrophes
	- easiest to keep them together
- Handling hyphenations
	- keep them together if they're part of a name
	- otherwise separate them
- Dealing with (other) special expressions (URLs, emails, numbers)

The key is to being **consistent** with the rest of the NLP system.It is about knowing **when to split, not when to combine**. You would rather split it up e.g. San Francisco and **NOT TO COMBINE**. You can combine later.

The next step is to **normalisation**.
### Normalisation:
Map tokens to *normalised* form:
- {walks, walked, walk, walking} $\longrightarrow$ walk
- {B.B.C., BBC} $\longrightarrow$ BBC
- {multi-national, multinational} $\longrightarrow$ multinational

There are two principal approaches:
- **Lemmatisation**
- **Stemming**
#### Case-Folding:
- Convert everything to lower/uppercase can make things more easier or difficult
- We may loose some meaning 

### Side Note: Character Based Tokenisation
We could use character based tokenisation using character **n-grams**. This forms the basic building blocks. We usually use Byte-Pair-Encoding(BPE):
- **Token learner**: takes a raw training corpus and **induces** a vocabulary 
- **Token segmenter**: takes a raw test i.e. input sentence and tokenisation it according to that vocabulary 

How does it work?
- We choose the two symbols that are most **frequently adjacent** in the training corpus e.g. 'A', 'B'
- Add a new merged symbol 'AB' to the vocabulary
- Replace every adjacent 'A' 'B' in the corpus with 'AB'.
- Until $\underline{K}$ merges has been done.
Note we add a special _ to show the space.
![[Screenshot 2023-10-13 at 22.38.11.png]]

![[Screenshot 2023-10-13 at 22.36.53.png]]

![[Screenshot 2023-10-13 at 22.37.14.png]]

![[Screenshot 2023-10-13 at 22.37.28.png]]

- Most words and subwords will be represented as full symbols
- Very rare tokens will be represented by their parts "subparts"
- Can control the parameter $\underline{K}$ 

- Now on the **test data** we can run each merger learnt form the training data greedily and **in the order we learnt them**.
	- Test word "newer_" would be tokenised as a full word
	- Test word "lower_" which was **NOT** seen in the train corpus, so it would be represented as two tokens: "low" and "er_"
- This reduces the number of unseen tokens(OOV)
## Approach 1: Lemmatisation:
**Lemmatisation** is the reduction to dictionary headword form.

First way is using a **dictionary**, and then **looking up** each word. It is a very **slow** process, and there are issues with words that are **NOT in the dictionary(OOV)**.
- {am, are, is} $\longrightarrow$ be
- {horse, horses, horse's, horses'} $\longrightarrow$ horse
- {life, lives} $\longrightarrow$ life

Second way is using **Morphological analysis.** Words have suffix, prefix for example unhappiness $\longrightarrow$ **un** + happy + **ness**


When we use words there are **modification** we do to the word into to convey meaning?  This process is know as **inflection**.
#### Inflection:
**Inflection**  is the **modification** of a word to express different grammatical categories, for example: 
- tense (past, present, future)
- number ($1^{st}$,  $2^{nd}$, $3^{rd}$)
- gender
- grammatical voice (active, passive)
e.g. comes $\longrightarrow$ come, dancing $\longrightarrow$ dance
There are also issues:
- performances $\longrightarrow$ performance
- performances $\longrightarrow$ perform
- There are issues of over/under normalise, so depends on how you do.

#### Stemming:
Stemming is where you **chop off words**.
- It removes the suffix, possible prefix.
- Quick fix.
This can largely **neutralise inflection** and **some derivation**.
- However it can yield words that are not words
	- e.g. automat**e**, automat**ic** $\longrightarrow$ automat.
- Under-stemming fails to conflate related form, as you didn't chop enough:
	- divide $\longrightarrow$ divid
	- division $\longrightarrow$ divis
- Over-stemming conflates unrelated form, by chopping too much:
	- neutron, neutral $\longrightarrow$ neutr 
	- experiment, experience $\longrightarrow$ experi 


