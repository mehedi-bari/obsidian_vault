 **Distributional hypothesis** suggests that one can infer the meaning of a word just looking at the context it occurs in. **Distributional semantics** we only consider its semantics not grammer.

The simplest way to do this is using vector space model (**VSM**). There are two different types: document-term matrix and term-context occurrences matrix.

Note: Word Vector and Word Embedding are the same thing
## VSM: 
Word vector by **documents**:
You can build your document-term matrix.
![[Screenshot 2023-11-03 at 18.36.13.png]]
Then you calculate the similarity between words using the word vectors (rows vectors in this case.)

Word vectors by **local context**:
Build your term-context matrix.
![ ]
Calculate the similarity between words using the word vectors(row vectors in this case.)
Co-occurrence of a team and a word (from the dictionary) within the **context windows** of the term, as observed in a text collection.

**Context-Window size** is very important: 
$\square$  Shorter window focuses on syntax
$\square$  Longer window captures the semantics
To handle large context window size, more expressive model, larger data and more computing are needed.
#### Sparse Vs. Dense Models:
The VSM generates high-dimensional sparse word vectors.
Alternatively, we would like to learn low-dimensional dense vectors:
$\square$  Comparatively low dimensions 
$\square$  Mostly non-zero elements
$\square$  This gives the benefit of being less noisy, and easier to be used as feature.

## Latent Semantic Indexing (LSI)
This is a **dense** model. This is the same as singular value decomposition.
![[Screenshot 2023-11-03 at 18.57.17.png]]

ยง

![[Screenshot 2023-11-03 at 20.19.37.png]]
There are 3 unique feature vectors or the $rank(M) = 3$ and we say there are 3 degrees of freedom. In this case, we can set k= 4 and 5 but we will waste computational power, and it is a waste of memory. However, we can set k = 1 or 2, but we will lose some information. We also may have limitations on the amount of computing power, so we can limit the dimensions. This is known as **truncated SVD.**

## Predictive Word Embedding Models:
This is a different way of calculating word vectors.
In this task, first we need to define the context. Then we usually have a target word, and we have a context window which contains some words that surround the target word, which are known as **context words**. So, the task would be to predict the target word with the context words. 
There are two types:
$\square$  word2vec: It is a classification task. It predicts whether a word appears in a context of a target word (based on CBOW
$\square$  GloVe: It is a regression task. It uses frequency in the context of texts of a target word.
### CBOW 
We know that a word that is being used will usually have a few context words in a context window which will be common. In addition, some words that are the same can have different meaning depending on the context. As a result, this method takes into account the context as the input, and uses SLP to compute the output. We can train the model depending on how close it was from getting to the context e.g. if we were trying to create to finish the sentence I like petting _cats, and it gives us _dogs_ it is getting close.
![[Screenshot 2023-11-06 at 23.08.41.png]]
In this example, we use the context word as features, and use the target word as the class. In this we have a feature vector, and we use **one-hot-encoding** to find a specific line. It can be trained on those parameters on the NN. The models gives us a probability of what it can be using **logistic regression**. 

## Skip gram model
This is the opposite of CBOW. It uses the target word as the feature, and use the context words as the class. So, you classify the target word into possible classess.
 ![[Screenshot 2023-11-13 at 19.02.52.png]]