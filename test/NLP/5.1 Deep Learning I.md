Deep Learning and Neural Network are often used interchangeably. 
	$\square$  When we talk about DL we use NN
	$\square$  It has many, many layers
	$\square$  We use end-to-end systems, which take the raw object as the input.

There are 4 deep learning task in NLP:
- **Sequence distribution**: Model a probability distribution of a sequence like $p(x_k | x_1, x_2,x_3,..., x_{k-1})$ or $p(x_1,x_2,x_3,...,x_{k-1},x_k)$ 
- **Sequence classification**: To learn a representation vector of a sequence and use if to classify the sequence e.g. $f(x_1 \longrightarrow x_2 \longrightarrow ... \longrightarrow x_{k-1}, \longrightarrow x_k) = class$. This is the as document classification, and figuring out which type of news category it belongs to.
- **Sequence labelling**: To learn a representation vector for each state(element) in a sequence, and use it to predict label for each state e.g.  $f(x_1 \longrightarrow x_2 \longrightarrow ... \longrightarrow x_{k-1}, \longrightarrow x_k) = class_1 \longrightarrow class_2 \longrightarrow .. \longrightarrow class_{k-1} \longrightarrow class_k$
- **Seq2Seq Learning**: It is to form a mapping from one sequence to another, which involves **encoding** the input, and then **decoding*** it to generate an output. e.g.   $f(x_1 \longrightarrow x_2 \longrightarrow ... \longrightarrow x_{k-1}, \longrightarrow x_k) =  y_1 \longrightarrow y_2 \longrightarrow ... \longrightarrow y_{k-1} \longrightarrow y_k$

## Sequence Probability 
Given a sequence of words, and we want to find the probability of then next word given the previous words. We can use **conditional probability** for to find the highest probability probability. However if we use neural networks, it is called neural language model. 
![[Screenshot 2023-11-06 at 21.51.41.png]]

## Sequence Classification:
The typical two task are: sentimental analysis, and spam filtering.
![[Screenshot 2023-11-06 at 21.55.09.png]]

## Sequence Labelling
Part-of-speech tagging: Assign a word to a priorly defined lexical class. So, we want to find out what class the word belongs to. We can integrate a higher level of complexity by considering the previous state. This is done using RNN.
![[Screenshot 2023-11-06 at 22.01.53.png]]
Named entity recognition: Assign a word to a name class(e.g. in a name class or in no name class.) PER:Person
![[Screenshot 2023-11-06 at 22.02.00.png]]


## Seq2seq learning
We can use seq2seq learning to compute machine translation. You want your encoder to extract most of the important information.
![[Screenshot 2023-11-06 at 22.06.56.png]]
It is also possible to do question answering, where it figures out the key information, and use the encoder and decoder to generate an answer.
Some other examples include: speech to text, picture to location.

## Recap of ML:
#### Single Neuron:
It is very similar to a linear a function. Neuron takes in  $d$  different input features, and it calculates a weighted sum.  Then an activation function is used with $b$ bias, to find the result of a single neuron. The activation function makes it different to a linear problem.
![[Screenshot 2023-11-06 at 22.18.22.png]]
We have many different activation function:
![[Screenshot 2023-11-06 at 22.22.12.png]]
#### Single Layer Perceptron(SLP):
It has one input layer and one output layer. This allow you to have multiple outputs. In essence, you can have one layer, and multiple inputs and output.
![[Screenshot 2023-11-06 at 22.28.06.png]]
#### Multilayer Perceptron:
A multilayer perceptron(MLP)/**feedforward neural network** consists of at least three layers of nodes: input, (at least one) hidden layer, and output layers. It is **fully connected** to all the neurones in the previous layer through non-zero weight.
![[Screenshot 2023-11-06 at 22.32.51.png]]
#### Neural Language Model
![[Screenshot 2023-11-06 at 22.38.32.png]]
$F$ is the data with sequence of words words, and $\mathbf{x_i}$ are the different **word vectors** which are acquired using the **one-hot-encoder**.  You pass it to a single layer perceptron, and also you pass the the **word vectors** to the next layer. The next layer processes it, and we usually use **logistic regression** to output our prediction in the prediction layer.


# Recurrent Neural Network (RNN)
![[Screenshot 2023-11-13 at 19.33.02.png]]
We usually use the **word vectors** as the input vectors, or we could use neural networks to extract features. We need an initial state,$h_0$, which can be trained using a neural network/fixed. We can calculate the next state $h_1 = f(\mathbf{x_1},\mathbf{h_0},W)$. then we can also calculate $h_2 = f(\mathbf{2_1},\mathbf{h_1},W)$, and $h_3 = f(\mathbf{x_3},\mathbf{h_2},W)$
> The representation vector $h_3$ encode the information provided by the the NL system.
> The same function is applied tin each state.
> The RNN parameters to be trained are stored in $W$ 
> RNN can handle **varying length sequences!**

Therefore a vanilla RNN uses a standard neuron operation to compute the hidden representation vector:
$$h_k=\varphi(W_xx_k +W_hh_{k-1}+b)$$
This can results in **vanishing gradient** problems, which negatively affects the training!
There are two way of solving this problem: **LSTM Cell** & **GRU Cell**.
![[Screenshot 2023-11-13 at 19.49.22.png]]

## LSTM (Long Short-Term Memory)
![[Screenshot 2023-11-13 at 19.50.46.png]]
$c_k$ is known as the **content vector**. 
It looks at the current state, and the previous state and chooses to ignore certain states using the output gate. This enables information to be passed down.

## GRU (Gated Recurrent Units)
![[Screenshot 2023-11-13 at 19.56.20.png]]

# Tagging Models based on RNN
![[Screenshot 2023-11-13 at 20.01.13.png]]

# Seq2Seq Model using RNN
 ![[Screenshot 2023-11-13 at 20.02.40.png]]

## Bidirectional RNN
![[Screenshot 2023-11-13 at 20.03.21.png]]
## Multi-Layer RNN
![[Screenshot 2023-11-13 at 20.05.22.png]]