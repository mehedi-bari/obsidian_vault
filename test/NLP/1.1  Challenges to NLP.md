What makes an application a language processing application (as opposed to any other piece of software)?
- Requires the use of knowledge about human language(s).

What's the issue with data?
Data are not typically unstructured. So therefore there are lots of difficulty in understanding what the text means.

Why is NLP hard?
- Variability: There are numerous ways to express the same thing
- Ambiguity: What does the words mean? Or what is it suggesting?
	- There could be word level ambiguity 
	- Also sentence level ambiguity.
	- Special expression like idioms, proverbs, irony
	- context interpretation
	- spoken language
	- size - large scale

What resources/data can we use in/for NLP?
- Dictionaries of words or phrases
	- With meaning, e.g. WordNet
	- With pronunciation
	- With what-goes-with-what
- Collection of texts or speech(corpora)
	- A **corpus** is a large collection of linguistic data, which may consist of written texts, spoken or written languages.
	- Language in use
	- Examples of sentences, documents, conversations.


#### Corpus annotation:
There are different types of corpora:
- so there are mono vs multi-lingual
- general, comparable and parallel etc.
- There can be unannotated - raw text/speech
- They can be annotated - enhanced with linguistic information.
	- Annotated labelled corpus is a repo of **explicit** linguistic information 

We can also verb subcategorisation frames:
![[Screenshot 2023-10-02 at 21.38.27.png]]

There are huge costs of manual annotation
- Can be prohibitively huge
- Can NLP "learn" without or with minimal annotations?
Issues in relation to the quality of annotations?
- Do people agree on the annotations?
- How consistent are they? This depends on how difficult the task is, and the clarity of **annotation guidelines** 

#### Annotation agreement
$\kappa$ measures the agreement between two annotators who each classify $N$ items into $C$ mutally exclusive categories
$$\kappa = \frac{Pr(a)-Pr(e)}{1-Pr(e)}$$
$Pr(a)$ = relative observed agreement among annotators.
$Pr(e)$ = the hypothetical probability of chance agreement
	-  Typically using the observed data to calculate the probabilities of each observer randomly saying each category,
If the annotators are in complete agreement then $\kappa$ = 1, and otherwise.


 ![[Screenshot 2023-10-02 at 21.59.36.png]]


![[Screenshot 2023-10-02 at 22.01.15.png]]![[Screenshot 2023-10-13 at 10.32.18.png]]
Both precision and recall test the capability of classifying the **positives** in a sample. 

Precision measures the accuracy of **false positives**. It answers the question of "Of all the instances predicted as positive, how many were correctly predicted?"

Recall measures the accuracy of **false negative**. It answers the question of "Of all the actual positive instances, how many were correctly predicted as positive by the model?"

$$F_1 =  \frac{2(Precision * Recall)}{Recall + Precision}$$




