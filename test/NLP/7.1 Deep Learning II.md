# Attention RNN
![[Screenshot 2023-11-13 at 20.23.28.png]]
To solve this problem we use **Attention RNN**.
![[Screenshot 2023-11-13 at 20.36.08.png]]
First we find the **similarity** between the two states $h_i$ and $\tilde{h_1}$. We use the **softmax** function to find $n$ probabilities, which are used to create a linear function to find $\tilde{h_1^a}.$
You can do it for all the other values:
![[Screenshot 2023-11-13 at 20.49.06.png]]

This helps:
- Improve the model **performance**
- Solves the **bottleneck** information problem
- Helps the **vanishing gradient problem**
- Provides **interpretability**. 

# Generalised Attention Formulation
This is called an **autoregressive structure.** Now we are going to simplify the model so it is easier to understand. 
![[Screenshot 2023-11-13 at 20.53.18.png]]
In this we store the encoded information in vector $V$ which are stored as $h_1,h_2,$ and $h_3$.  Also the decoded information in matrix $Q$, which are $\tilde{h_1},\tilde{h_2}$ and $\tilde{h_3}$. Then we compute the attention using the previous algorithm using $Z= softmax(QV^T)V$  to in find the $Z$ matrix which has $\hat{h_1}, \hat{h_2}$ and $\hat{h_3}$. This works by applying the softmax function to each row of the matrix.

Even simplified form:
![[Screenshot 2023-11-13 at 21.21.51.png]]
- Input: Query, Key, and Values matrices $(Q,K,V)$ defined by the model.
- Computer the connections between pairs of objects in the two sets using $q_i$ and $k_j$
- In this case $K$ and $Q$ can be different and they can have different representations, and add more information but the size `m` needs to be the same. 
- Note $K$ is used to find the **similarity** and $V$ is used for the **weighted sum**
$$Z=attention(Q,K,V) \longrightarrow Z= softmax(\frac{QK^T}{\sqrt{d_k}})
V$$
ยง
## Mutli-head attention 
![[Screenshot 2023-11-13 at 21.37.37.png]]
Suppose you have $\mathbf{Para} = \{ \{ W^Q_i \}^H_{i=1}, \{ W^K_i \}^H_{i=1}, \{ W^V_i \}^H_{i=1}\}$  which are all the different **weights** for the different $(Q,K,V)$ and we als have it for each `i`. This means we can train it `n` times and form a $[Z_1, Z_2,...,Z_n]$. Note each $Z_i$ has $Hd$ dimensions, and there are `n` of them. so we use a matrix transformation $W_0$ which is $Hd \times d_{model}$ and we can can calculate $\mathbf{Z}= [Z_1, Z_2,...,Z_n]W^0$ and $\mathbf{Z}$ has dimensions $n \times d_{model}$ 
![[Screenshot 2023-11-13 at 21.47.03.png]]

# Transformer
It contains fours parts: 
- positional encoding
- encoder, decoder, encoder-decoder
- add and layer normalisation
- fully-connected feedforward NN

