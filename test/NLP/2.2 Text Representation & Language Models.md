---
~
---
 A **Model** is an abstract representation of something, often in a computational form.
We usually use a **Bag of Words** representation, which could words or n-grams. 
There are some things to consider:
- Do we use all words?
	- The **stop-words** do we remove them.
	- The most frequent works?
- Do we want to rank them?
	- Give some **weight** to more representative, so more important. 
	- Then how do you decide on importance/relevance?

## Zip's Law:
Zip's law suggests that the frequency of any word in a given collection is **inversely proportional** to its rank in the frequency table. So, if you do $rank \times freq$, then you get a rough *constant*.
![[Screenshot 2023-10-13 at 23.18.31.png]]

### Luhn's hypothesis:
The words that are very frequent are not very representative of the corpus. The words are very very rare might not help you understand the text. So we want to look at the words in the middle.
![[Screenshot 2023-10-13 at 23.21.53.png]]

## Vector Representation:
The vector can be used to represent frequency, weight, or incidence (0,1).
**Dimension** is the size of the vocabulary.
![[Screenshot 2023-10-13 at 23.26.32.png]]
###### Is frequency vector a better indicator?
It is a good indicator of whether something is important or not, but it is **NOT** good for weights. For example, it is 10 times more important because it is 10 times for frequent. 
Note: Rare terms have low frequency but are most discriminative than frequent terms.

### Inverse Document Frequency
Document frequency ($df_t$) is the number of documents that contain the term $t$. $df_t$ is an inverse of the **informativeness** of $t$.
We define the idf(inverse document frequency) of $t$ as
$$idf_t = log_{10}(\frac{N}{df_t})$$
where $N$ ~ total number of documents, **NOT** $|V|$ ~ total number of terms
- High for terms that appear in few documents.
- Low for terms that appear in many documents.

The **tf-idf** weight of a term is the product of its $tf$ weight and its **idf**.
$$tf.idf_{t,d} = (1+\log_{10}tf_{t,d}) \times \log_{10} (\frac{N}{df_t})$$
We add **+1** for smoothing!  Note this has two parameters $t$ and $d$. Also, $tf.idf_{t,d}$  a measure of the importance of the **weight of a term in a document**. Whereas, $df_t$ is a value assigned to a term in the **whole collection**, it is **NOT** related to a document.

This increases with:
- with number of occurrences within a document
- with the rarity of the term in the collection.

![[Screenshot 2023-10-13 at 23.51.31.png]]
So, we can use tf$*$idf  instead of frequency.
However, this doesn't solve the issue of **sparsity**. We normally use **embedding** to solve that.
![[Screenshot 2023-10-13 at 23.54.23.png]]

## N-grams
$\mathbfit{N}$-grams are a sequence of words, where $\mathbfit{N}$ ~ number of words.
![[Screenshot 2023-10-13 at 23.56.57.png]]
This can help us solve two different problems: we can represent **context** and we can also preserve **word order**. This has a few costs, like if $\mathbfit{N}$ becomes larger the  $\mathbfit{N}$-grams are going to be **less frequent** and **more sparse**.

## Probabilistic Language Models:
We can use uni-grams to consider a sequence of independent uno-grams. This requires the uni-grams to **independently** & **iid**.
$p(S) = p(w_1) \times p(w_2) \times ... \times w_n = \prod_ip(w_i)$
![[Screenshot 2023-10-14 at 00.05.36.png]]

However, it doesn't work that well, because different sentence can have the same probabilities.
e.g. $p$("the cat in the hat $<$s$>$") > $p$("the hat in the cat $<$/s$>$") 
#### How do we do it?
Use **corpus frequency** as relative counts - count(#) how many times they appear in a real-world dataset. 
How do we deal with OOV words? $p(OOV) = 0$ 
$$ P(w) = \frac{\#w + 1}{\sum_{w^\prime \in D} (\#w^\prime +1)}$$
The **+1** is there for smoothing.
#### Chain rule:
$$ p(w_1...w_n) = p(w_1)p(w_2|w_1)...p(w_n|w_1,...,w_{n-1}) = \prod^i_{k=1}p(w_k|w_1,w_2..w_n)$$
###### Markov Assumption:
We can simplify the above assumption to $p$(hat| the cat in the) $\approx$ $p$(hat | the)
Or we could use two previous words $p$(hat| the cat in the) $\approx$ $p$(hat| in the)
This is the same as the bigram or the unigram model displayed below.
![[Screenshot 2023-10-14 at 15.38.49.png]]

#### N-gram Probability:
How to estimate the N-gram conditional probability using a training corpus.
$\square$  Based on word counts
$\square$  Using a statistical model e.g. hidden Markov model, and Gaussian mixture model.
$\square$  Using a machine learning mode e.g. neural network

For the bigram case:
$$p(w_k,w_{k-1}) = \frac{count(w_{k-1}w_k)}{\sum_w count(w_{k-1}w_k)} = \frac{count(w_{k-1}w_k)}{count(w_{k-1})} = \frac{C(w_{k-1;k})}{C(w_{k-1})}$$

![[Screenshot 2023-10-14 at 15.52.24.png]]

Answers:
$\square$   $p(<$s$>) |$ Sam $) =$ $\frac{1}{2}$               $\square$  $p($ not | do $)=$ $\frac{1}{1} = 1$
$\square$  $p($Sam | am $) =$ $\frac{1}{2}$                    $\square$  $p($eggs | green) = $\frac{1}{1} = 1$
$\square$  $p($ do | I $)=$ $\frac{1}{3}$ 

#### Count-Based Estimation:
![[Screenshot 2023-10-14 at 16.00.39.png]]
After normalisation:
![[Screenshot 2023-10-14 at 16.05.18.png]]

This is a very **simple** model, and very easy to construct these $\mathbf{N}$-gram models, but they don't always give very good answers. They're dependent on things like the corpus it has been trained on. Sometimes we have **long-distance dependencies** it could be very poor.  

## Evaluation: How good is our model?
We train parameters of our model on the training set. Then we test our model's performance on unseen data, which is different to our training data. We need an **evaluation metric** to tell us how well our model does on the test set.

We can use the **Shannon game**:
$\square$  We can come up with sentence, and see how well our model predicts the missing words.

A better language model of a text:
$\square$  It is one which assign a higher probability to the word that actually occurs
$\square$  Unigram are terrible in the game - it depends on the corpus' frequency of each word.


### Extrinsic Evaluation of LMs:
We try to find the best evaluation for **comparing** models A and B.
$\square$   Put each model through a test e.g. spelling corrector, speech, recogniser, MT system.
$\square$  Run the task, get an accuracy for A and B e.g. how many misspelled words, how many words were translated properly.
$\square$  Compare accuracy for A and B
This is **NOT** easy, and it can be  **very time consuming**.

The best language model is one that best predicts and unseen test set, however it doesn't tell us **how useful the model is**.
$\square$  Generally only useful in pilot experiment
$\square$  Bad approximation unless the test data set looks like the training data.

The perils of overfitting!
What happens when you have words that doesn't appear in the training corpus? We have talked about **smoothing** but this raises an issue too. All unknown words will have the same probability.

###### Side Note: MLE:
E.g. suppose the word “bagel” occurs 400 times in a corpus of a million words What is the probability that a random word from some other text will be “bagel”? 
MLE estimate is 400/1,000,000 = .0004
This may be a bad estimate for some other corpus • But it is the estimate that makes it most likely that “bagel” will occur 400 times in a million word corpus.


## Backoff and Interpolation
Sometimes it helps to use less context. 
**BackOff**:
$\square$  Use trigrams if you have good evidence.
$\square$ Otherwise use bigrams, otherwise unigrams.
**Interpolation**: 
$\square$  you can mix unigrams, bigrams, and trigrams, which usually works better.
$\square$  for example $\hat{p}(w_n|w_{n-2}w_{n-1}) = \lambda_1 (w_n|w_{n-2}w_{n-1}) + \lambda_2 p(w_n|w_{n-1}) + \lambda_3 p(w_n)$ 

## Training for unknown words: 
-  Create an unknown word token `<UNK>` 
- Training of probabilities • 
	- Create a fixed lexicon `L` of size `V` 
	- At text normalisation phase, any training word not in L changed to `<UNK>`
	- [Or, replace some random words as `<UNK>`]
	- Now we train its probabilities (for `<UNK>`) like a normal word
- At decoding time: 


