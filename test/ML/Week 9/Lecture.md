![[Screenshot 2023-05-13 at 17.57.43.png]]










True


![[Screenshot 2023-05-13 at 17.59.30.png]]









True








![[Screenshot 2023-05-13 at 18.00.40.png]]










a datapoint lying on the supporting hyperplane of the decision boundary




















![[Screenshot 2023-05-13 at 18.02.16.png]]










at least 2 support vectors










![[Screenshot 2023-05-13 at 18.03.38.png]]









Decision boundary in SVM is not determined by all the training points, but soley on the closest points to the margin,









![[Screenshot 2023-05-13 at 18.05.42.png]]






True






![[Screenshot 2023-05-13 at 18.06.29.png]]





Your data is not linearly separable.




![[Screenshot 2023-05-13 at 18.10.32.png]]






The k-nn will perform best, since it is a non-inear model.





![[Screenshot 2023-05-13 at 18.13.11.png]]






To constrain the optimisation to ensure correct classification.





![[Screenshot 2023-05-13 at 18.14.54.png]]




False






![[Screenshot 2023-05-13 at 18.16.07.png]]








The fact that we can compute an implicit high dimensional feature space via  a kernal function




The three defining aspects of an SVM are most accuractely described as:
1) it optimises slackvariables, it minimisesthe margin error, andit computes thekernel via a ‘trick’.
2) it optimises slackvariables, it minimisesthe margin error, andit computes thekernel via a ‘trick’.
3) it maximises themargin, it computesan implicit highdimensional spacevia a kernel, and itallows for nonlinearproblems via slackvariables.
4) it uses a hinge lossfunction, it computesslack features, and itprojects into a highdimensional space.




its E (rotate)






![[Screenshot 2023-05-13 at 18.22.42.png]]
